# Deadline Cloud Job Attachments - Snapshots API Summary

**Date:** 2026-01-10  
**Commits:** 8527ba37..c45b04ca (97 commits)

## Summary

This change introduces a comprehensive "Snapshots" sub-library within Deadline Cloud job attachments that provides composable manifest operations for capturing directory tree snapshots, computing diffs, and performing uploads/downloads to content-addressable storage. The design separates concerns into distinct operations (COLLECT, HASH, HASH_UPLOAD, DOWNLOAD, FILTER, DIFF, COMPOSE, SUBTREE, PARTITION, JOIN) that can be combined for various workflows like job submission, output download, and debug snapshot creation. A new manifest format (v2025-12) adds support for directories, symlinks, chunked large files, and diff manifests with deletion markers. The implementation uses pipelined parallel processing for hash+upload and download operations with memory-bounded execution.

---

## New Manifest Format (v2025-12)

### Specification Versions
- `absolute-manifest-snapshot-beta-2025-12` - Full snapshot with absolute paths
- `absolute-manifest-diff-beta-2025-12` - Diff manifest with absolute paths  
- `relative-manifest-snapshot-beta-2025-12` - Full snapshot with relative paths
- `relative-manifest-diff-beta-2025-12` - Diff manifest with relative paths

### Manifest Classes (In-Memory)

```
Snapshot        - Relative-path snapshot (portable format)
SnapshotDiff    - Relative-path diff between snapshots
AbsSnapshot     - Absolute-path snapshot (for filesystem operations)
AbsSnapshotDiff - Absolute-path diff (for applying changes)
```

### Type Aliases
```python
RelManifest = Union[Snapshot, SnapshotDiff]      # Any relative-path manifest
AbsManifest = Union[AbsSnapshot, AbsSnapshotDiff] # Any absolute-path manifest
AnySnapshot = Union[AbsSnapshot, Snapshot]        # Any snapshot
AnyDiff = Union[AbsSnapshotDiff, SnapshotDiff]    # Any diff
AnyManifest = Union[...]                          # All four types
```

### ManifestFilePath Fields
| Field | Type | Description |
|-------|------|-------------|
| `path` | `str` | File path (POSIX format, `/` separator) |
| `hash` | `Optional[str]` | Content hash (None for symlinks, chunked, deleted, or unhashed) |
| `size` | `Optional[int]` | File size in bytes |
| `mtime` | `Optional[int]` | Modification time in microseconds |
| `runnable` | `bool` | POSIX execute bit |
| `chunkhashes` | `Optional[List[str]]` | Chunk hashes for files > 256MB |
| `symlink_target` | `Optional[str]` | Target path for symlinks |
| `deleted` | `bool` | Deletion marker for diff manifests |

### ManifestDirectoryPath Fields
| Field | Type | Description |
|-------|------|-------------|
| `path` | `str` | Directory path |
| `deleted` | `bool` | Deletion marker for diff manifests |

### Constants
- `DEFAULT_FILE_CHUNK_SIZE = 256 * 1024 * 1024` (256MB)
- `WHOLE_FILE_CHUNK_SIZE = -1` (no chunking sentinel)

### JSON Encoding Features
- Directory index compression: `$N/filename` references parent directory by index
- Canonical JSON output (sorted keys, no whitespace)
- Auto-collection of parent directories during encode

---

## New Public APIs

### Module: `deadline.job_attachments._snapshots`

All operations are exported from `_snapshots/__init__.py`:

```python
from deadline.job_attachments._snapshots import (
    # Manifest classes
    Snapshot, SnapshotDiff, AbsSnapshot, AbsSnapshotDiff,
    # Type aliases
    RelManifest, AbsManifest, AnySnapshot, AnyDiff, AnyManifest,
    # Path classes
    ManifestDirectoryPath, ManifestFilePath,
    # Constants
    DEFAULT_FILE_CHUNK_SIZE, WHOLE_FILE_CHUNK_SIZE,
    # Data caches
    ContentAddressedDataCache, S3DataCache, FileSystemDataCache,
    # Operations
    collect_manifest, hash_manifest, hash_upload_manifest,
    download_manifest, DownloadResult,
    filter_manifest, compute_diff_manifest, compose_manifests,
    subtree_manifest, partition_manifest, join_manifest,
    # Enums
    SymlinkPolicy, IncludeExcludePathsFilter,
)
```

---

## Operations API Reference

### 1. COLLECT: `collect_manifest()`

**Signature:**
```python
def collect_manifest(
    directories: List[Path | str],
    filenames: List[Path | str],
    *,
    optional_filenames: Optional[List[Path | str]] = None,
    symlink_policy: SymlinkPolicy = SymlinkPolicy.COLLAPSE_ESCAPING,
    file_chunk_size_bytes: Optional[int] = None,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> AbsSnapshot:
```

**Purpose:** Scan directories and files into a manifest WITHOUT computing hashes.

**Call Stack:**
```
collect_manifest()
├── _collect_manifest_impl()
│   ├── Path.stat(follow_symlinks=False)     # FS: Get file metadata
│   ├── os.walk(dir, followlinks=...)        # FS: Traverse directory tree
│   ├── _create_unhashed_file_entry()        # Create ManifestFilePath with hash=None
│   ├── _handle_symlink()                    # Process symlink per policy
│   │   └── os.readlink()                    # FS: Read symlink target
│   └── _collect_escaping_dir_symlink()      # Collapse escaping dir symlinks
└── AbsSnapshot(...)                         # Return manifest with absolute paths
```

**SymlinkPolicy Enum:**
| Policy | Description |
|--------|-------------|
| `COLLAPSE_ESCAPING` | Preserve internal symlinks, collapse escaping ones (default) |
| `COLLAPSE_ALL` | Follow all symlinks as files/directories |
| `PRESERVE` | Keep all symlinks with absolute targets |
| `TRANSITIVE_INCLUDE_TARGETS` | Keep symlinks and add their targets |
| `EXCLUDE_ALL` | Skip all symlinks |
| `EXCLUDE_ESCAPING` | Preserve internal, exclude escaping symlinks |

---

### 2. HASH: `hash_manifest()`

**Signature:**
```python
def hash_manifest(
    manifest: AbsManifest,
    hash_cache: Optional[HashCache] = None,
    force_rehash: bool = False,
    file_chunk_size_bytes: Optional[int] = None,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> AbsManifest:
```

**Purpose:** Fill in hashes for files in a manifest (collected with hash=None).

**Call Stack:**
```
hash_manifest()
├── _validate_absolute_paths()
├── For each file entry:
│   ├── _get_or_compute_hash()
│   │   ├── HashCache.get_entry()            # Check cache by (path, mtime)
│   │   └── hash_data() / _hash_file_chunked()
│   │       └── open(file, 'rb').read()      # FS: Read file content
│   │       └── xxhash.xxh128()              # Compute XXH128 hash
│   └── HashCache.put_entry()                # Update cache
└── AbsManifest(...)                         # Return manifest with hashes
```

---

### 3. HASH_UPLOAD: `hash_upload_manifest()`

**Signature:**
```python
def hash_upload_manifest(
    manifest: AbsManifest,
    data_cache: ContentAddressedDataCache,
    *,
    hash_cache: Optional[HashCache] = None,
    s3_check_cache: Optional[S3CheckCache] = None,
    account_id: Optional[str] = None,
    max_workers: int = DEFAULT_MAX_WORKERS,
    max_memory_bytes: Optional[int] = None,
    progress_tracker: Optional[ProgressTracker] = None,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> AbsManifest:
```

**Purpose:** Hash files AND upload to data cache in a pipelined manner.

**Call Stack:**
```
hash_upload_manifest()
├── _validate_absolute_paths()
├── _get_default_max_memory_bytes()          # psutil.virtual_memory()
├── _TaskBasedPipeline(...)
│   ├── ThreadPoolExecutor (READ+HASH pool)
│   └── ThreadPoolExecutor (UPLOAD pool)
├── _MemoryPool(max_bytes)                   # Backpressure control
├── For each file:
│   └── pipeline.submit(_ChunkWorkItem | _StreamingWorkItem)
│       ├── _do_read_and_hash()              # READ+HASH stage
│       │   ├── _memory_pool.allocate()      # Block if memory exhausted
│       │   ├── open(file, 'rb').read()      # FS: Read chunk
│       │   └── hash_data()                  # Hash while in memory
│       └── _do_upload()                     # UPLOAD stage
│           ├── S3DataCache:
│           │   ├── s3_client.head_object()  # AWS: Check existence
│           │   └── s3_client.put_object()   # AWS: Upload content
│           └── FileSystemDataCache:
│               └── open(path, 'wb').write() # FS: Write to cache
│           └── _memory_pool.release()       # Free memory
├── pipeline.wait_for_completion()
└── AbsManifest(...)                         # Return manifest with hashes
```

**AWS API Usage:**
- `s3:HeadObject` - Check if object exists in S3
- `s3:PutObject` - Upload file content to S3

---

### 4. DOWNLOAD: `download_manifest()`

**Signature:**
```python
def download_manifest(
    manifest: AbsManifest,
    data_cache: ContentAddressedDataCache,
    *,
    hash_cache: Optional[HashCache] = None,
    file_conflict_resolution: FileConflictResolution = FileConflictResolution.SKIP,
    max_workers: int = DEFAULT_MAX_WORKERS,
    progress_tracker: Optional[ProgressTracker] = None,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> DownloadResult:
```

**Purpose:** Download files from data cache to local filesystem.

**Call Stack:**
```
download_manifest()
├── _validate_absolute_paths()
├── Create all directories first:
│   └── os.makedirs(dir, exist_ok=True)      # FS: Create directory tree
├── asyncio.run(_download_all_files_async())
│   ├── For regular files:
│   │   └── _download_single_file_async()
│   │       ├── _check_hash_cache_for_skip() # Check if file up-to-date
│   │       ├── preallocate_file()           # FS: Sparse file allocation
│   │       ├── S3DataCache:
│   │       │   ├── download_s3_multipart_async() (large files)
│   │       │   │   └── s3_client.get_object(Range=...) # AWS: Byte-range GET
│   │       │   └── s3_client.get_object()   # AWS: Full GET (small files)
│   │       └── FileSystemDataCache:
│   │           └── shutil.copy2()           # FS: Copy from cache
│   │       └── os.replace(temp, final)      # FS: Atomic move
│   │       └── os.utime()                   # FS: Restore mtime
│   ├── For chunked files:
│   │   └── _download_chunked_file_async()
│   │       ├── preallocate_file()           # FS: Pre-allocate full size
│   │       ├── For each chunk (parallel):
│   │       │   └── download_s3_chunk_to_offset() / download_fs_chunk_to_offset()
│   │       │       └── pwrite() at offset   # FS: Write chunk at offset
│   │       └── os.replace() + os.utime()    # FS: Finalize
│   └── For symlinks:
│       └── os.symlink(target, path)         # FS: Create symlink
├── Apply deletions (for diff manifests):
│   ├── os.unlink(file)                      # FS: Delete file
│   └── os.rmdir(dir)                        # FS: Delete empty directory
└── DownloadResult(statistics, updated_manifest)
```

**AWS API Usage:**
- `s3:GetObject` - Download file content (with optional Range header for multipart)

---

### 5. FILTER: `filter_manifest()`

**Signature:**
```python
def filter_manifest(
    manifest: AnyManifest,
    filter_func: Callable[[ManifestFilePath], bool],
    *,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> AnyManifest:
```

**Purpose:** Filter manifest entries using a callable predicate.

**Call Stack:**
```
filter_manifest()
├── For each file entry:
│   └── filter_func(entry)                   # User-provided predicate
├── For each directory:
│   └── Check if any file under it passes    # Keep dirs with matching files
└── AnyManifest(...)                         # Return filtered manifest
```

**Helper Class:** `IncludeExcludePathsFilter` - Glob-based include/exclude filtering

---

### 6. DIFF: `compute_diff_manifest()`

**Signature:**
```python
def compute_diff_manifest(
    parent: AnySnapshot,
    current: AnySnapshot,
    parent_manifest_hash: Optional[str] = None,
    ignore_hashes: bool = False,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
    *,
    preserve_runnable: bool = False,
) -> AnyDiff:
```

**Purpose:** Compute difference between two snapshots.

**Call Stack:**
```
compute_diff_manifest()
├── Build path lookups:
│   ├── parent_file_paths = {path: entry for entry in parent.files}
│   └── current_file_paths = {path: entry for entry in current.files}
├── Find changes:
│   ├── new_files = current_paths - parent_paths
│   ├── deleted_files = parent_paths - current_paths
│   └── modified_files = [p for p in common if _entries_differ(parent[p], current[p])]
├── Ensure deleted dirs have contents deleted
└── AnyDiff(files=[new + modified + deleted_markers], dirs=[...])
```

---

### 7. COMPOSE: `compose_manifests()`

**Signature:**
```python
def compose_manifests(
    manifests: List[AnyManifest],
    *,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> AnyManifest:
```

**Purpose:** Layer manifests together (later entries override earlier).

**Call Stack:**
```
compose_manifests()
├── _ManifestTrieNode()                      # Build trie structure
├── For each manifest in order:
│   ├── For each file entry:
│   │   ├── trie.insert_path(components)     # Add/update entry
│   │   └── If deleted: trie.delete_if_empty() # Apply deletion
│   └── For each directory:
│       └── trie.insert_path() or delete_if_empty()
├── trie.iter_files()                        # Collect final entries
└── AnyManifest(...)                         # Return composed manifest
```

---

### 8. SUBTREE: `subtree_manifest()`

**Signature:**
```python
def subtree_manifest(
    manifest: AnyManifest,
    subtree: str,
    *,
    symlink_policy: SymlinkPolicy = SymlinkPolicy.COLLAPSE_ESCAPING,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> RelManifest:
```

**Purpose:** Extract a subtree as a relative-path manifest.

**Call Stack:**
```
subtree_manifest()
├── _normalize_subtree_path()
├── _validate_path_style_consistency()
├── For each entry:
│   ├── _is_within_subtree(path, subtree)    # Filter to subtree
│   └── _rebase_path(path, subtree)          # Strip prefix
├── Handle symlinks per symlink_policy
└── RelManifest(...)                         # Return relative-path manifest
```

---

### 9. PARTITION: `partition_manifest()`

**Signature:**
```python
def partition_manifest(
    manifest: AnyManifest,
    roots: Optional[List[str]] = None,
    *,
    referenced_paths: Optional[List[str]] = None,
    symlink_policy: SymlinkPolicy = SymlinkPolicy.COLLAPSE_ESCAPING,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> List[Tuple[str, RelManifest]]:
```

**Purpose:** Split manifest into multiple (root, RelManifest) pairs.

**Call Stack:**
```
partition_manifest()
├── _validate_roots_no_overlap()
├── _determine_all_roots()                   # Auto-detect roots if not provided
│   ├── POSIX: longest common prefix
│   └── Windows: one root per drive letter
├── For each root:
│   └── subtree_manifest(manifest, root, symlink_policy)
└── List[(root, RelManifest)]
```

---

### 10. JOIN: `join_manifest()`

**Signature:**
```python
def join_manifest(
    manifest: RelManifest,
    prefix: str,
    *,
    print_function_callback: Callable[[Any], None] = lambda msg: None,
) -> AnyManifest:
```

**Purpose:** Prepend a prefix to all paths (inverse of SUBTREE).

**Call Stack:**
```
join_manifest()
├── For each file entry:
│   └── new_path = f"{prefix}/{entry.path}"
├── For each directory:
│   └── new_path = f"{prefix}/{dir.path}"
└── AnyManifest(...)                         # Abs if prefix is absolute
```

---

## Data Cache Classes

### ContentAddressedDataCache (Abstract Base)

```python
@dataclass
class ContentAddressedDataCache(ABC):
    @abstractmethod
    def get_object_key(self, hash_value: str, algorithm: str) -> str: ...
    @abstractmethod
    def object_exists(self, hash_value: str, algorithm: str) -> bool: ...
```

### S3DataCache

```python
@dataclass
class S3DataCache(ContentAddressedDataCache):
    s3_bucket: str
    s3_key_prefix: str
    s3_client: Any  # boto3 S3 client
    s3_check_cache: Optional[S3CheckCache] = None
```

**Key format:** `{s3_key_prefix}/{hash}.{algorithm}` (e.g., `Data/abc123.xxh128`)

### FileSystemDataCache

```python
@dataclass
class FileSystemDataCache(ContentAddressedDataCache):
    root_path: Path  # Must be absolute
```

**Key format:** `{root_path}/{hash}.{algorithm}` (e.g., `/mnt/cache/abc123.xxh128`)

---

## Component Layer Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              CLI / Public APIs                               │
│  deadline bundle submit | deadline job download-output | deadline queue ...  │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                         Composable Operations Layer                          │
│  collect_manifest | hash_manifest | hash_upload_manifest | download_manifest │
│  filter_manifest | compute_diff_manifest | compose_manifests                 │
│  subtree_manifest | partition_manifest | join_manifest                       │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                    ┌─────────────────┼─────────────────┐
                    ▼                 ▼                 ▼
┌───────────────────────┐ ┌───────────────────┐ ┌───────────────────────────┐
│   Manifest Classes    │ │   Data Caches     │ │   Pipeline Infrastructure │
│  Snapshot             │ │  S3DataCache      │ │  _TaskBasedPipeline       │
│  SnapshotDiff         │ │  FileSystemData   │ │  _MemoryPool              │
│  AbsSnapshot          │ │    Cache          │ │  ThreadPoolExecutor       │
│  AbsSnapshotDiff      │ │                   │ │  asyncio coordination     │
│  ManifestFilePath     │ │                   │ │                           │
│  ManifestDirectoryPath│ │                   │ │                           │
└───────────────────────┘ └───────────────────┘ └───────────────────────────┘
                    │                 │                 │
                    ▼                 ▼                 ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Caching Layer                                      │
│              HashCache (file hash by path+mtime+range)                       │
│              S3CheckCache (S3 object existence)                              │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                    ┌─────────────────┼─────────────────┐
                    ▼                 ▼                 ▼
┌───────────────────────┐ ┌───────────────────┐ ┌───────────────────────────┐
│   File System APIs    │ │   AWS S3 APIs     │ │   Hashing                 │
│  os.stat/walk/readlink│ │  GetObject        │ │  xxhash.xxh128()          │
│  open/read/write      │ │  PutObject        │ │  hash_data()              │
│  os.replace/utime     │ │  HeadObject       │ │                           │
│  os.symlink/unlink    │ │                   │ │                           │
│  preallocate_file()   │ │                   │ │                           │
└───────────────────────┘ └───────────────────┘ └───────────────────────────┘
```

---

## Internal APIs (Prefixed with `_`)

### Manifest Module (`_manifest.py`)
- `_normalize_path_in_manifest(path: str) -> str`
- `_is_absolute_path(path: str) -> bool`
- `_BaseManifest` - Base class with common fields
- `_AbsManifestMixin` - Validates absolute paths
- `_RelManifestMixin` - Validates relative paths
- `_SnapshotManifestMixin` - Validates no deleted entries
- `_DiffManifestMixin` - Validates diff constraints

### Collect Operation (`_collect_manifest.py`)
- `_collect_manifest_impl()` - Core collection logic
- `_create_unhashed_file_entry()` - Create file entry with hash=None
- `_handle_symlink()` - Process symlink per policy
- `_get_symlink_absolute_target()` - Resolve symlink target
- `_symlink_target_is_directory()` - Check if symlink points to directory
- `_collect_escaping_dir_symlink()` - Collapse escaping directory symlinks
- `_collect_transitive_target()` - Add transitive symlink targets

### Hash Upload Operation (`_hash_upload_manifest.py`)
- `_validate_absolute_paths()` - Ensure manifest has absolute paths
- `_get_default_max_memory_bytes()` - Calculate memory limit from system
- `_ChunkWorkItem` - Work item for chunk-based processing
- `_StreamingWorkItem` - Work item for large files
- `_FileResult` - Result of processing a single file
- `_MemoryPool` - Thread-safe memory allocation with backpressure
- `_TaskBasedPipeline` - Two-pool pipeline (READ+HASH, UPLOAD)
- `_TaskBasedPipeline._do_read_and_hash()` - Combined read+hash stage
- `_TaskBasedPipeline._do_upload()` - Upload stage
- `_TaskBasedPipeline._stream_hash_file()` - Stream hash for large files
- `_TaskBasedPipeline._upload_chunk_to_s3()` - Upload chunk to S3
- `_TaskBasedPipeline._upload_chunk_to_filesystem()` - Upload chunk to FS
- `_TaskBasedPipeline._stream_upload_to_s3()` - Stream upload to S3
- `_TaskBasedPipeline._stream_upload_to_filesystem()` - Stream upload to FS

### Download Operation (`_download_manifest.py`)
- `_validate_absolute_paths()` - Ensure manifest has absolute paths
- `_check_hash_cache_for_skip()` - Check if file already up-to-date
- `_check_hash_cache_for_chunked_skip()` - Check chunked file cache
- `_update_hash_cache_for_chunked_file()` - Update cache after download
- `_get_new_copy_file_path()` - Generate unique path for conflicts
- `_download_single_file_async()` - Download regular file with multipart
- `_download_chunk_to_offset()` - Download chunk to file offset
- `_download_chunked_file_async()` - Download chunked file in parallel

### Download S3 Helpers (`_download_manifest_s3.py`)
- `download_s3_multipart_async()` - Parallel byte-range downloads
- `download_s3_chunk_to_offset()` - Download S3 chunk to offset
- `download_s3_chunk_multipart_async()` - Multipart chunk download

### Download FS Helpers (`_download_manifest_file_system.py`)
- `download_fs_chunk_to_offset()` - Copy chunk from FS cache

### Sparse File Utilities (`_sparse_file.py`)
- `preallocate_file()` - OS-specific sparse file allocation

### Diff Operation (`_diff_manifest.py`)
- `_entries_differ()` - Compare two file entries
- `_compute_diff_manifest()` - Core diff computation

### Compose Operation (`_compose_manifest.py`)
- `_ManifestTrieNode` - Trie node for efficient composition
- `_ManifestTrieNode.insert_path()` - Navigate/create path
- `_ManifestTrieNode.delete_subtree()` - Delete entire subtree
- `_ManifestTrieNode.delete_if_empty()` - Delete empty directory
- `_ManifestTrieNode.mark_deleted()` - Mark path as deleted
- `_ManifestTrieNode.iter_files()` - Iterate file entries
- `_ManifestTrieNode.iter_deleted_files()` - Iterate deleted entries

### Subtree Operation (`_subtree_manifest.py`)
- `_normalize_subtree_path()` - Normalize path separators
- `_validate_path_style_consistency()` - Check path style match
- `_is_within_subtree()` - Check if path is under subtree
- `_rebase_path()` - Strip subtree prefix from path
- `_identity_subtree_manifest()` - Apply symlink policy without rebasing
- `_subtree_manifest()` - Core subtree extraction

### Partition Operation (`_partition_manifest.py`)
- `_normalize_path()` - Normalize path for comparison
- `_validate_roots_no_overlap()` - Ensure no root is subpath of another
- `_collect_all_dirs()` - Get all directories from manifest
- `_determine_all_roots()` - Auto-detect roots if not provided
- `_find_common_prefix()` - Find longest common path prefix (POSIX)
- `_find_windows_roots()` - Find drive letter roots (Windows)

### Join Operation (`_join_manifest.py`)
- `_join_manifest()` - Core join implementation

### V2025 Encode/Decode (`asset_manifests/_v2025_12/`)
- `encode_v2025()` - Serialize manifest to JSON
- `decode_v2025()` - Deserialize JSON to manifest
- `_get_specification_version()` - Map class to spec version
- `_collect_all_directories()` - Collect dirs for encoding
- `_sort_and_dedupe_dirs()` - Sort and validate directories
- `_sort_and_dedupe_files()` - Sort and validate files
- `_encode_dirs()` - Encode directories with $N/ compression
- `_encode_files()` - Encode files with $N/ compression
- `_build_dir_index()` - Build index for $N/ expansion
- `_decode_dirs()` - Decode directory entries
- `_decode_files()` - Decode file entries
- `_expand_path_reference()` - Expand $N/ references
- `validate_manifest_2025_12()` - Validate JSON structure

---

## Use Case Workflows

### 1. Job Submission (`deadline bundle submit`)
```
COLLECT(directories, filenames, symlink_policy=COLLAPSE_ESCAPING)
    → AbsSnapshot (hash=None)
HASH_UPLOAD(manifest, S3DataCache)
    → AbsSnapshot (with hashes, data uploaded)
PARTITION(manifest)
    → List[(root, RelManifest)]  # For CreateJob API
```

### 2. Output Download (`deadline job download-output`)
```
For each task output manifest:
    JOIN(manifest, absolute_prefix)
        → AbsManifest
Sort by S3 last-modified timestamp
COMPOSE(manifests)
    → AbsManifest (merged)
DOWNLOAD(manifest, S3DataCache)
    → DownloadResult
```

### 3. Debug Snapshot (`deadline bundle submit --save-debug-snapshot`)
```
COLLECT(directories, filenames)
    → AbsSnapshot
HASH_UPLOAD(manifest, FileSystemDataCache(local_path))
    → AbsSnapshot (data written to local cache)
PARTITION(manifest)
    → List[(root, RelManifest)]
# Zip the local cache + manifests
```

### 4. Incremental Sync
```
COLLECT(directory)
    → current: AbsSnapshot (hash=None)
HASH(current, hash_cache)
    → current: AbsSnapshot (with hashes)
DIFF(parent, current)
    → diff: AbsSnapshotDiff
HASH_UPLOAD(diff, S3DataCache)
    → diff: AbsSnapshotDiff (uploaded)
```

---

## Key Design Decisions

1. **Separation of COLLECT, HASH, HASH_UPLOAD** - Enables fast diff by mtime/size, hash cache integration, and reduced redundant reads.

2. **Absolute paths for filesystem operations** - Simplifies implementation; use SUBTREE/JOIN for conversion.

3. **V2023 format via lossy conversion** - Symlinks collapsed/excluded, empty dirs dropped, deletions dropped, chunk size must be WHOLE_FILE_CHUNK_SIZE.

4. **POSIX path separators always** - Even on Windows, paths use `/`. Windows backslashes converted on input.

5. **Symlink targets always absolute or relative to manifest root** - Not relative to symlink's parent directory.

6. **Directory deletions require content deletion first** - Ensures composability of diff manifests.

7. **Pipelined hash+upload** - Separate thread pools for READ+HASH and UPLOAD prevent deadlock; memory pool provides backpressure.

8. **Parallel chunked downloads** - Each chunk downloaded independently to pre-allocated offset; asyncio coordinates finalization.
